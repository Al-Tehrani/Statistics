{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/banner.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Accuracy of Regression Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lecture, we learned about the concept of regression and how to find the best-fitting line using the least squares method. We also discussed the assumptions of linearity and homoscedasticity that should be met for the regression model to be valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a regression line, it's crucial to evaluate how well it fits the data and how accurate its predictions are. In this lecture, we will explore various methods to assess the accuracy and reliability of our regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by introducing the **standard error of estimate**, denoted as $s_{y|x}$, which measures the average distance between the observed values and the predicted values from the regression line. This metric provides an overall measure of the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will dive into the role of the **correlation coefficient** ($r$) in determining the strength and direction of the linear relationship between the variables. We will see how the square of the correlation coefficient, known as the **coefficient of determination** ($r^2$), represents the proportion of variance in the dependent variable that can be explained by the independent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will discuss the interpretation of $r^2$ and its limitations, emphasizing that a high $r^2$ value does not necessarily imply causation between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will introduce the concept of **multiple regression**, where more than one independent variable is used to predict the dependent variable. We will briefly discuss how multiple regression extends the concepts learned in simple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this lecture, you will have a solid understanding of how to evaluate the accuracy of regression predictions and interpret the key metrics associated with the regression model. This knowledge will enable you to make informed decisions when applying regression analysis to real-world problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive in and explore the fascinating world of regression accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Standard Error of Estimate ($s_{y|x}$)](#toc1_)    \n",
    "  - [Definition and Formula](#toc1_1_)    \n",
    "  - [Interpreting the Standard Error of Estimate](#toc1_2_)    \n",
    "  - [Example Calculation](#toc1_3_)    \n",
    "- [The Role of the Correlation Coefficient ($r$)](#toc2_)    \n",
    "  - [Revisiting the Correlation Coefficient ($r$)](#toc2_1_)    \n",
    "  - [The Relationship Between $r$ and the Accuracy of Predictions](#toc2_2_)    \n",
    "  - [Examples of How $r$ Affects the Sum of Squares for Predictive Errors](#toc2_3_)    \n",
    "- [The Coefficient of Determination ($r^2$)](#toc3_)    \n",
    "  - [Definition and Interpretation of $r^2$](#toc3_1_)    \n",
    "  - [Calculation of $r^2$ and the Proportion of Predicted Variability](#toc3_2_)    \n",
    "  - [Examples of Calculating and Interpreting $r^2$](#toc3_3_)    \n",
    "  - [Limitations and Misconceptions about $r^2$](#toc3_4_)    \n",
    "- [Comparing Predictive Accuracy: $r^2$ vs. $s_{y|x}$](#toc4_)    \n",
    "  - [The Relationship Between $r^2$ and $s_{y|x}$](#toc4_1_)    \n",
    "  - [When to Use $r^2$ and When to Use $s_{y|x}$](#toc4_2_)    \n",
    "  - [Advantages and Disadvantages of Each Measure](#toc4_3_)    \n",
    "- [Introduction to Multiple Regression Equations](#toc5_)    \n",
    "  - [Common Features of Multiple Regression Equations](#toc5_1_)    \n",
    "  - [Brief Overview of How Multiple Regression Can Improve Predictive Accuracy](#toc5_2_)    \n",
    "- [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Standard Error of Estimate ($s_{y|x}$)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of estimate, denoted as $s_{y|x}$, is a measure of the average distance between the observed values of the dependent variable ($y$) and the predicted values ($\\hat{y}$) obtained from the regression line. It provides an estimate of the average prediction error and is **expressed in the same units as the dependent variable**.\n",
    "\n",
    "Note that while the formula may look similar to standard deviation, the standard error of estimate and the standard deviation serve different purposes. The standard error of estimate measures the accuracy of predictions made by a regression line, while the standard deviation measures the amount of variation in a population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Definition and Formula](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of estimate is calculated using the following formula:\n",
    "\n",
    "$s_{y|x} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - 2}}$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the observed value of the dependent variable for the $i$-th observation\n",
    "- $\\hat{y}_i$ is the predicted value of the dependent variable for the $i$-th observation\n",
    "- $n$ is the total number of observations\n",
    "- The denominator $(n - 2)$ represents the degrees of freedom, which is the number of observations minus the number of parameters estimated. In simple linear regression, any straight line can be made to coincide with two data points. Therefore, the degrees of freedom are $n - 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/standard-error-of-estimate.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Interpreting the Standard Error of Estimate](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of estimate provides valuable information about the accuracy of the regression model:\n",
    "\n",
    "- A smaller standard error of estimate indicates that the predicted values are closer to the observed values, suggesting a better fit of the regression line to the data.\n",
    "- A larger standard error of estimate suggests that the predicted values are farther from the observed values, indicating a poorer fit of the regression line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of estimate can be used to construct prediction intervals around the predicted values. For example, assuming the residuals are normally distributed, approximately 95% of the observed values are expected to fall within $\\pm 2 \\times s_{y|x}$ of the predicted values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Example Calculation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the standard error of estimate for the example data from the previous lecture:\n",
    "\n",
    "| Hours Studied ($x$) | Exam Score ($y$) |\n",
    "|---------------------|------------------|\n",
    "| 2.5                 | 58               |\n",
    "| 3.2                 | 67               |\n",
    "| 4.1                 | 73               |\n",
    "| 5.0                 | 78               |\n",
    "| 5.8                 | 85               |\n",
    "| 6.7                 | 88               |\n",
    "| 7.4                 | 91               |\n",
    "| 8.2                 | 96               |\n",
    "| 9.1                 | 99               |\n",
    "| 9.9                 | 102              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the least squares regression line $\\hat{y} = 43.02 + 5.74x$ from the previous lecture, we can calculate the predicted values $\\hat{y}_i$ for each observation and then find the squared residuals $(y_i - \\hat{y}_i)^2$:\n",
    "\n",
    "| $x_i$ | $y_i$ | $\\hat{y}_i$ | $(y_i - \\hat{y}_i)^2$ |\n",
    "|-------|-------|-------------|----------------------|\n",
    "| 2.5   | 58    | 57.37       | 0.40                 |\n",
    "| 3.2   | 67    | 61.39       | 31.47                |\n",
    "| 4.1   | 73    | 66.55       | 41.60                |\n",
    "| 5.0   | 78    | 71.72       | 39.42                |\n",
    "| 5.8   | 85    | 76.31       | 75.43                |\n",
    "| 6.7   | 88    | 81.48       | 42.64                |\n",
    "| 7.4   | 91    | 85.50       | 30.25                |\n",
    "| 8.2   | 96    | 90.09       | 34.93                |\n",
    "| 9.1   | 99    | 95.25       | 14.06                |\n",
    "| 9.9   | 102   | 99.85       | 4.62                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the squared residuals is $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = 314.82$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the standard error of estimate:\n",
    "\n",
    "$s_{y|x} = \\sqrt{\\frac{314.82}{10 - 2}} = \\sqrt{39.35} \\approx 6.27$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of estimate for this regression model is approximately 6.27, which means that, on average, the predicted exam scores deviate from the observed exam scores by about 6.27 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will discuss the correlation coefficient and the coefficient of determination, which provide further insights into the strength and accuracy of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[The Role of the Correlation Coefficient ($r$)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will revisit the concept of the correlation coefficient ($r$) and explore its role in determining the accuracy of regression predictions. The correlation coefficient measures the strength and direction of the linear relationship between the independent variable ($x$) and the dependent variable ($y$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Revisiting the Correlation Coefficient ($r$)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient ($r$) ranges from -1 to +1:\n",
    "\n",
    "- A value of $r$ close to +1 indicates a strong positive linear relationship, meaning that as $x$ increases, $y$ tends to increase.\n",
    "- A value of $r$ close to -1 indicates a strong negative linear relationship, meaning that as $x$ increases, $y$ tends to decrease.\n",
    "- A value of $r$ close to 0 indicates a weak or no linear relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for calculating the correlation coefficient is:\n",
    "\n",
    "$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$\n",
    "\n",
    "where:\n",
    "- $x_i$ and $y_i$ are the individual values of the independent and dependent variables\n",
    "- $\\bar{x}$ and $\\bar{y}$ are the means of the independent and dependent variables\n",
    "- $n$ is the total number of observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_'></a>[The Relationship Between $r$ and the Accuracy of Predictions](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient ($r$) plays a crucial role in determining the accuracy of regression predictions. A stronger linear relationship between the variables, indicated by a value of $r$ closer to -1 or +1, generally leads to more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $r$ is close to -1 or +1, the data points tend to fall closer to the regression line, resulting in smaller residuals (differences between the observed and predicted values). Consequently, the sum of squared residuals, $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, tends to be smaller, indicating a better fit of the regression line to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, when $r$ is close to 0, the data points are more scattered around the regression line, leading to larger residuals and a higher sum of squared residuals. In this case, the regression line may not provide accurate predictions, as the relationship between the variables is weak or nonexistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_'></a>[Examples of How $r$ Affects the Sum of Squares for Predictive Errors](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To derive the formula for the standard error of estimate using the correlation coefficient, we start with the original formula and then substitute the definition of the correlation coefficient. Let's go through this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original formula for the standard error of estimate:\n",
    "\n",
    "$S_{y|x} = \\sqrt{\\frac{\\sum(Y - \\hat{Y})^2}{n - 2}}$\n",
    "\n",
    "Where:\n",
    "- $S_{\\varepsilon}$ is the standard error of estimate\n",
    "- $Y$ is an actual score\n",
    "- $\\hat{Y}$ is a predicted score\n",
    "- $n$ is the number of observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient $(r)$ is defined as:\n",
    "\n",
    "$r = \\frac{\\sum(X - \\bar{X})(Y - \\bar{Y})}{\\sqrt{\\sum(X - \\bar{X})^2\\sum(Y - \\bar{Y})^2}}$\n",
    "\n",
    "Where:\n",
    "- $X$ is an independent variable\n",
    "- $\\bar{X}$ is the mean of the independent variable\n",
    "- $Y$ is the dependent variable\n",
    "- $\\bar{Y}$ is the mean of the dependent variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1:** Square both sides of the correlation coefficient formula and solve for the sum of squares of the residuals $\\sum(Y - \\hat{Y})^2$.\n",
    "\n",
    "$r^2 = \\frac{[\\sum(X - \\bar{X})(Y - \\bar{Y})]^2}{\\sum(X - \\bar{X})^2\\sum(Y - \\bar{Y})^2}$\n",
    "\n",
    "$r^2\\sum(Y - \\bar{Y})^2 = \\sum(Y - \\hat{Y})^2$\n",
    "\n",
    "$\\sum(Y - \\hat{Y})^2 = (1 - r^2)\\sum(Y - \\bar{Y})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2:** Substitute this expression into the original formula for the standard error of estimate.\n",
    "\n",
    "$S_{y|x} = \\sqrt{\\frac{(1 - r^2)\\sum(Y - \\bar{Y})^2}{n - 2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 3:** Recognize that $\\sqrt{\\frac{\\sum(Y - \\bar{Y})^2}{n - 1}}$ is the formula for the sample standard deviation of $Y$ $(s_y)$. Substitute this into the formula.\n",
    "\n",
    "$S_{y|x} = \\sqrt{(1 - r^2)\\frac{(n - 1)s_y^2}{n - 2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 4:** *Simplify the formula by pulling out the square root and cancelling the $(n - 1)$ term.*\n",
    "\n",
    "$S_{y|x} = SS_y\\sqrt{\\frac{(n - 1)(1 - r^2)}{n - 2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 5:** **For large samples**, $\\frac{n - 1}{n - 2} \\approx 1$, so we can simplify the formula further.\n",
    "\n",
    "$S_{y|x} \\approx SS_y\\sqrt{1 - r^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the standard error of estimate can be approximated using the correlation coefficient and the standard deviation of the dependent variable:\n",
    "\n",
    "$S_{y|x} \\approx SS_y\\sqrt{1 - r^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the impact of the correlation coefficient on the accuracy of predictions, let's consider some examples using the sum of squares for predictive errors, denoted as $SS_{y|x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $SS_{y|x}$ can be calculated using the following formula:\n",
    "\n",
    "$SS_{y|x} = (1 - r^2) \\times SS_y$\n",
    "\n",
    "where $SS_y$ is the total sum of squares for the dependent variable, calculated as $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Perfect positive correlation ($r = 1$)\n",
    "- If $r = 1$, then $SS_{y|x} = (1 - 1^2) \\times SS_y = 0$. This means that when there is a perfect positive linear relationship between the variables, the regression line fits the data perfectly, and there are no predictive errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Perfect negative correlation ($r = -1$)\n",
    "- If $r = -1$, then $SS_{y|x} = (1 - (-1)^2) \\times SS_y = 0$. Similar to the previous example, a perfect negative linear relationship results in a perfect fit of the regression line and no predictive errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: No correlation ($r = 0$)\n",
    "- If $r = 0$, then $SS_{y|x} = (1 - 0^2) \\times SS_y = SS_y$. When there is no linear relationship between the variables, the sum of squares for predictive errors is equal to the total sum of squares for the dependent variable. In this case, the regression line does not provide any predictive value, and the predictions are no better than using the mean of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples demonstrate that as the absolute value of the correlation coefficient ($|r|$) increases, the sum of squares for predictive errors decreases, indicating more accurate predictions. Conversely, as $|r|$ approaches 0, the sum of squares for predictive errors increases, suggesting less accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore the coefficient of determination ($r^2$), which provides further insights into the proportion of variance in the dependent variable that can be explained by the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[The Coefficient of Determination ($r^2$)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination, denoted as $r^2$, is a statistical measure that represents the proportion of variance in the dependent variable ($y$) that can be explained by the independent variable ($x$) in a linear regression model. It provides insights into the goodness of fit and the predictive power of the regression line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Definition and Interpretation of $r^2$](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination is the square of the correlation coefficient ($r$). It ranges from 0 to 1, where:\n",
    "\n",
    "- A value of $r^2$ close to 1 indicates that a large proportion of the variance in $y$ can be explained by $x$, suggesting a strong linear relationship and a good fit of the regression line to the data.\n",
    "- A value of $r^2$ close to 0 indicates that a small proportion of the variance in $y$ can be explained by $x$, suggesting a weak linear relationship and a poor fit of the regression line to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of $r^2$ is straightforward: it represents the percentage of the variance in the dependent variable that can be explained by the independent variable. For example, if $r^2 = 0.75$, it means that 75% of the variance in $y$ can be explained by $x$, while the remaining 25% is unexplained or attributed to other factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_'></a>[Calculation of $r^2$ and the Proportion of Predicted Variability](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination can be calculated using the following formula:\n",
    "\n",
    "$r^2 = \\frac{SS_{regression}}{SS_{total}} = 1 - \\frac{SS_{residual}}{SS_{total}}$\n",
    "\n",
    "where:\n",
    "- $SS_{regression}$ is the sum of squares for regression, representing the variability in $y$ that can be explained by $x$. It is calculated as $\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2$.\n",
    "- $SS_{residual}$ is the sum of squares for residuals, representing the variability in $y$ that cannot be explained by $x$. It is calculated as $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$.\n",
    "- $SS_{total}$ is the total sum of squares, representing the total variability in $y$. It is calculated as $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of predicted variability can be calculated by multiplying $r^2$ by 100%. For example, if $r^2 = 0.75$, then 75% of the variability in $y$ can be predicted by $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_'></a>[Examples of Calculating and Interpreting $r^2$](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an example to illustrate the calculation and interpretation of $r^2$. Suppose we have the following data:\n",
    "\n",
    "| $x$ | $y$ |\n",
    "|-----|-----|\n",
    "| 1   | 3   |\n",
    "| 2   | 5   |\n",
    "| 3   | 7   |\n",
    "| 4   | 9   |\n",
    "| 5   | 11  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the formulas for the least squares regression line, we find that $\\hat{y} = 1 + 2x$. Now, let's calculate $r^2$:\n",
    "\n",
    "- $SS_{regression} = (3 - 7)^2 + (5 - 7)^2 + (7 - 7)^2 + (9 - 7)^2 + (11 - 7)^2 = 40$\n",
    "- $SS_{residual} = (3 - 3)^2 + (5 - 5)^2 + (7 - 7)^2 + (9 - 9)^2 + (11 - 11)^2 = 0$\n",
    "- $SS_{total} = (3 - 7)^2 + (5 - 7)^2 + (7 - 7)^2 + (9 - 7)^2 + (11 - 7)^2 = 40$\n",
    "\n",
    "- $r^2 = \\frac{40}{40} = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, $r^2 = 1$, indicating that 100% of the variance in $y$ can be explained by $x$. This is a perfect linear relationship, and the regression line fits the data exactly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_'></a>[Limitations and Misconceptions about $r^2$](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While $r^2$ is a useful measure of the goodness of fit and predictive power of a regression model, it has some limitations and can be subject to misconceptions:\n",
    "\n",
    "1. **Correlation does not imply causation**: A high value of $r^2$ does not necessarily mean that changes in $x$ cause changes in $y$. There may be other factors or confounding variables that influence the relationship between $x$ and $y$.\n",
    "\n",
    "2. **Sensitivity to outliers**: $r^2$ can be greatly affected by outliers, which are data points that are significantly different from the rest of the data. Outliers can artificially inflate or deflate the value of $r^2$, leading to misleading interpretations.\n",
    "\n",
    "3. **Lack of consideration for other factors**: $r^2$ only considers the linear relationship between $x$ and $y$. It does not account for other factors that may influence $y$, such as non-linear relationships, interactions with other variables, or omitted variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential to interpret $r^2$ in the context of the specific problem and data, and to be aware of its limitations when drawing conclusions from a regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Comparing Predictive Accuracy: $r^2$ vs. $s_{y|x}$](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the accuracy of a regression model, two commonly used measures are the coefficient of determination ($r^2$) and the standard error of estimate ($s_{y|x}$). While both provide insights into the model's predictive power, they differ in their interpretation and use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[The Relationship Between $r^2$ and $s_{y|x}$](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r^2$ and $s_{y|x}$ are related but distinct measures of predictive accuracy:\n",
    "\n",
    "- $r^2$ represents the proportion of variance in the dependent variable ($y$) that can be explained by the independent variable ($x$). It focuses on the strength of the linear relationship between the variables and the goodness of fit of the regression line.\n",
    "\n",
    "- $s_{y|x}$ represents the average distance between the observed values of $y$ and the predicted values from the regression line. It focuses on the typical size of the prediction errors and is expressed in the same units as the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $r^2$ increases, $s_{y|x}$ tends to decrease, indicating better predictive accuracy. However, this relationship is not perfect, and it is possible to have a high $r^2$ value with a relatively large $s_{y|x}$, or vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[When to Use $r^2$ and When to Use $s_{y|x}$](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice between using $r^2$ or $s_{y|x}$ depends on the purpose of the analysis and the desired interpretation:\n",
    "\n",
    "1. **Use $r^2$ when**:\n",
    "   - You want to assess the strength of the linear relationship between the variables.\n",
    "   - You want to determine the proportion of variance in $y$ that can be explained by $x$.\n",
    "   - You want to compare the predictive power of different regression models, especially when the dependent variables are measured on different scales.\n",
    "\n",
    "2. **Use $s_{y|x}$ when**:\n",
    "   - You want to assess the typical size of the prediction errors in the original units of the dependent variable.\n",
    "   - You want to construct prediction intervals around the predicted values.\n",
    "   - You want to communicate the accuracy of the predictions to a non-technical audience, as $s_{y|x}$ is more easily interpretable than $r^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is often beneficial to consider both $r^2$ and $s_{y|x}$ when evaluating a regression model, as they provide complementary information about the model's predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Introduction to Multiple Regression Equations](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections, we focused on simple linear regression, which involves using a single independent variable to predict a dependent variable. However, in many real-world situations, a dependent variable may be influenced by multiple independent variables. This is where multiple regression comes into play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/multiple-regression.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression is an extension of simple linear regression that allows us to use two or more independent variables to predict a dependent variable. The general form of a multiple regression equation is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted value of the dependent variable\n",
    "- $\\beta_0$ is the y-intercept (the value of $y$ when all independent variables are zero)\n",
    "- $\\beta_1, \\beta_2, ..., \\beta_p$ are the regression coefficients for each independent variable\n",
    "- $x_1, x_2, ..., x_p$ are the values of the independent variables\n",
    "- $p$ is the number of independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each regression coefficient ($\\beta_i$) represents the change in the dependent variable for a one-unit increase in the corresponding independent variable, holding all other independent variables constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Common Features of Multiple Regression Equations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression equations share many features with simple linear regression equations:\n",
    "\n",
    "1. **Linearity**: The relationship between the dependent variable and each independent variable is assumed to be linear, holding all other independent variables constant.\n",
    "\n",
    "2. **Least squares estimation**: The regression coefficients are estimated using the least squares method, which minimizes the sum of squared residuals.\n",
    "\n",
    "3. **Assumptions**: Multiple regression relies on similar assumptions as simple linear regression, including linearity, independence of errors, homoscedasticity, and normality of errors.\n",
    "\n",
    "4. **Interpretation**: The interpretation of the regression coefficients is similar to simple linear regression, but it is important to consider the effects of each independent variable while holding others constant.\n",
    "\n",
    "5. **Predictive accuracy**: The accuracy of multiple regression predictions can be assessed using measures such as the coefficient of determination ($R^2$) and the standard error of estimate ($s_{y|x_1,x_2,...,x_p}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_'></a>[How Multiple Regression Can Improve Predictive Accuracy](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Multiple regression can improve predictive accuracy compared to simple linear regression by:\n",
    "\n",
    "1. **Accounting for multiple influences**: By including multiple independent variables, multiple regression can capture the combined effects of different factors on the dependent variable, providing a more comprehensive model.\n",
    "\n",
    "2. **Capturing interactions**: Multiple regression can include interaction terms, which represent the combined effect of two or more independent variables on the dependent variable. This allows for more complex and realistic relationships to be modeled.\n",
    "\n",
    "3. **Improved $R^2$ and reduced $s_{y|x}$**: By including additional relevant independent variables, multiple regression can increase the proportion of variance explained ($R^2$) and reduce the standard error of estimate ($s_{y|x}$), indicating better predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is important to note that adding more independent variables does not always improve the model's predictive accuracy. Including irrelevant or redundant variables can lead to overfitting, increased complexity, and reduced interpretability. It is crucial to carefully select independent variables based on their theoretical relevance, statistical significance, and practical considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will explore some examples of multiple regression equations and discuss how to interpret their coefficients and assess their predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Conclusion](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we have explored various methods and concepts for evaluating the accuracy of regression predictions. Let's recap the key points:\n",
    "\n",
    "1. **Standard Error of Estimate ($s_{y|x}$)**: A measure of the average distance between the observed and predicted values, with a smaller $s_{y|x}$ indicating better predictive accuracy.\n",
    "\n",
    "2. **Correlation Coefficient ($r$) and Coefficient of Determination ($r^2$)**: $r$ determines the strength and direction of the linear relationship, while $r^2$ represents the proportion of variance in the dependent variable explained by the independent variable.\n",
    "\n",
    "3. **Limitations of $r^2$**: $r^2$ has limitations, such as lack of causality, sensitivity to outliers, and the need to consider other factors beyond the linear relationship.\n",
    "\n",
    "4. **Comparing Predictive Accuracy: $r^2$ vs. $s_{y|x}$**: $r^2$ focuses on the strength of the linear relationship and the proportion of variance explained, while $s_{y|x}$ provides information about the typical size of prediction errors.\n",
    "\n",
    "5. **Introduction to Multiple Regression**: Multiple regression allows for the inclusion of multiple independent variables to predict a dependent variable, potentially improving predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the accuracy of regression predictions is crucial for assessing the reliability of the model, comparing different models, and communicating results. It has important implications for decision-making, model refinement, and guiding future research efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By understanding and applying these concepts, we can make more informed decisions, refine our models, and continue to explore the world of regression analysis. As we move forward, it is essential to keep these principles in mind and use them to tackle real-world problems effectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
